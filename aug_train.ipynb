{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 adv_train.py --batch_size 128 --arch trades-wrn --dataset cifar \n",
    "# --attack \"FastLagrangePerceptualAttack(model, bound=0.5, num_iterations=10)\" --only_attack_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to simulate command line arguments\n",
    "\n",
    "\n",
    "args_dict = {\n",
    "    'include_checkpoint': True,  # Set to True or False based on your needs\n",
    "    'checkpoint': 'data/checkpoints/wrn_40_2.pt',  # You can set a default value if needed\n",
    "    # 'checkpoint': None,\n",
    "    'arch': 'trades-wrn',\n",
    "    'dataset': 'cifar',\n",
    "    'dataset_path': '~/datasets',\n",
    "    'num_epochs': 200,\n",
    "    'batch_size': 128,\n",
    "    'val_batches': 10,\n",
    "    'log_dir': 'data/logs',\n",
    "    'parallel': 1,\n",
    "    'lpips_model': None,\n",
    "    'only_attack_correct': True,\n",
    "    'randomize_attack': False,\n",
    "    'maximize_attack': False,\n",
    "    'seed': 0,\n",
    "    'continue': False,\n",
    "    'keep_every': 1,\n",
    "    'optim': 'sgd',\n",
    "    'lr': 0.1,\n",
    "    'lr_schedule': None,\n",
    "    'clip_grad': 1.0,\n",
    "    'attack': [\"00\"] # [\"FastLagrangePerceptualAttack(model, bound=0.5, num_iterations=10)\"]\n",
    "}\n",
    "\n",
    "# Create a namespace object from the dictionary\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True\n",
      "{'include_checkpoint': True, 'checkpoint': 'data/checkpoints/wrn_40_2.pt', 'arch': 'trades-wrn', 'dataset': 'cifar', 'dataset_path': '~/datasets', 'num_epochs': 200, 'batch_size': 128, 'val_batches': 10, 'log_dir': 'data/logs', 'parallel': 1, 'lpips_model': None, 'only_attack_correct': True, 'randomize_attack': False, 'maximize_attack': False, 'seed': 0, 'continue': False, 'keep_every': 1, 'optim': 'sgd', 'lr': 0.1, 'lr_schedule': '80,140', 'clip_grad': 1.0, 'attack': ['00']}\n",
      "tensor([0.4914, 0.4822, 0.4465]) tensor([0.2470, 0.2435, 0.2616])\n",
      "==> Preparing dataset cifar..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 194\u001b[0m\n\u001b[1;32m    192\u001b[0m     device_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mparallel))\n\u001b[1;32m    193\u001b[0m     model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model, device_ids)\n\u001b[0;32m--> 194\u001b[0m     attacks \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattack\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattacks\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    195\u001b[0m     validation_attacks \u001b[38;5;241m=\u001b[39m [nn\u001b[38;5;241m.\u001b[39mDataParallel(attack, device_ids)\n\u001b[1;32m    196\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m attack \u001b[38;5;129;01min\u001b[39;00m validation_attacks]\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# necessary to put training loop in a function because otherwise we get\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# huge memory leaks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 194\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m     device_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mparallel))\n\u001b[1;32m    193\u001b[0m     model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model, device_ids)\n\u001b[0;32m--> 194\u001b[0m     attacks \u001b[38;5;241m=\u001b[39m [\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m attack \u001b[38;5;129;01min\u001b[39;00m attacks]\n\u001b[1;32m    195\u001b[0m     validation_attacks \u001b[38;5;241m=\u001b[39m [nn\u001b[38;5;241m.\u001b[39mDataParallel(attack, device_ids)\n\u001b[1;32m    196\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m attack \u001b[38;5;129;01min\u001b[39;00m validation_attacks]\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# necessary to put training loop in a function because otherwise we get\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# huge memory leaks\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/adv_train/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:162\u001b[0m, in \u001b[0;36mDataParallel.__init__\u001b[0;34m(self, module, device_ids, output_device, dim)\u001b[0m\n\u001b[1;32m    159\u001b[0m     _check_balance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from typing import Any, Callable, List, Optional, cast\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from torch import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from perceptual_advex import evaluation\n",
    "from perceptual_advex.utilities import add_dataset_model_arguments, \\\n",
    "     get_dataset_model, calculate_accuracy\n",
    "from perceptual_advex.attacks import *\n",
    "from perceptual_advex.models import FeatureModel\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "VAL_ITERS = 100\n",
    "\n",
    "\n",
    "print(\"Cuda:\", torch.cuda.is_available())\n",
    "\n",
    "# Create a namespace from the dictionary\n",
    "args = Namespace(**args_dict)\n",
    "\n",
    "# Now you can access the arguments as if they were parsed from the command lin\n",
    "\n",
    "if args.optim == 'adam':\n",
    "    if args.lr is None:\n",
    "        args.lr = 1e-3\n",
    "    if args.lr_schedule is None:\n",
    "        args.lr_schedule = '120'\n",
    "    if args.num_epochs is None:\n",
    "        args.num_epochs = 100\n",
    "elif args.optim == 'sgd':\n",
    "    if args.dataset.startswith('cifar'):\n",
    "        if \"resnet\" in args.arch:\n",
    "            if args.lr is None:\n",
    "                args.lr = 1e-1\n",
    "            if args.lr_schedule is None:\n",
    "                args.lr_schedule = '75,90,100'\n",
    "            if args.num_epochs is None:\n",
    "                args.num_epochs = 100\n",
    "        elif args.arch == \"trades-wrn\":\n",
    "            if args.lr is None:\n",
    "                args.lr = 1e-1\n",
    "            if args.lr_schedule is None:\n",
    "                args.lr_schedule = '80,140'\n",
    "            if args.num_epochs is None:\n",
    "                args.num_epochs = 200\n",
    "        else:\n",
    "            print(args.arch, \"for cifar dataset!!!!\")\n",
    "    \n",
    "    elif (\n",
    "        args.dataset.startswith('imagenet')\n",
    "        or args.dataset == 'bird_or_bicycle'\n",
    "    ):\n",
    "        if args.lr is None:\n",
    "            args.lr = 1e-1\n",
    "        if args.lr_schedule is None:\n",
    "            args.lr_schedule = '30,60,80'\n",
    "        if args.num_epochs is None:\n",
    "            args.num_epochs = 90\n",
    "\n",
    "print(vars(args))\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "dataset, model = get_dataset_model(args)\n",
    "dataset.transform_train = transforms.Compose([\n",
    "                            v2.AutoAugment(v2.AutoAugmentPolicy.CIFAR10),\n",
    "                            transforms.ToTensor(),\n",
    "                        ])\n",
    "print(dataset.mean, dataset.std)\n",
    "\n",
    "if isinstance(model, FeatureModel):\n",
    "    model.allow_train()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "if args.lpips_model is not None:\n",
    "    _, lpips_model = get_dataset_model(\n",
    "        args, checkpoint_fname=args.lpips_model)\n",
    "    if torch.cuda.is_available():\n",
    "        lpips_model.cuda()\n",
    "\n",
    "train_loader, val_loader = dataset.make_loaders(\n",
    "    workers=4, batch_size=args.batch_size)\n",
    "\n",
    "attacks = [eval(attack_str) for attack_str in args.attack]\n",
    "validation_attacks = [\n",
    "    NoAttack(),\n",
    "    LinfAttack(model, dataset_name=args.dataset,\n",
    "               num_iterations=VAL_ITERS),\n",
    "    L2Attack(model, dataset_name=args.dataset,\n",
    "             num_iterations=VAL_ITERS),\n",
    "    JPEGLinfAttack(model, dataset_name=args.dataset,\n",
    "                   num_iterations=VAL_ITERS),\n",
    "    FogAttack(model, dataset_name=args.dataset,\n",
    "              num_iterations=VAL_ITERS),\n",
    "    StAdvAttack(model, num_iterations=VAL_ITERS),\n",
    "    ReColorAdvAttack(model, num_iterations=VAL_ITERS),\n",
    "    LagrangePerceptualAttack(model, num_iterations=30),\n",
    "]\n",
    "\n",
    "flags = []\n",
    "if args.only_attack_correct:\n",
    "    flags.append('only_attack_correct')\n",
    "if args.randomize_attack:\n",
    "    flags.append('random')\n",
    "if args.maximize_attack:\n",
    "    flags.append('maximum')\n",
    "if args.lpips_model:\n",
    "    lpips_model_name, _ = os.path.splitext(os.path.basename(\n",
    "        args.lpips_model))\n",
    "    flags.append(lpips_model_name)\n",
    "\n",
    "experiment_path_parts = [args.dataset, args.arch]\n",
    "if args.optim != 'sgd':\n",
    "    experiment_path_parts.append(args.optim)\n",
    "attacks_part = '-'.join(args.attack + flags)\n",
    "if len(attacks_part) > 255:\n",
    "    attacks_part = (\n",
    "        attacks_part\n",
    "        .replace('model, ', '')\n",
    "        .replace(\"'imagenet100', \", '')\n",
    "        .replace(\"'cifar', \", '')\n",
    "        .replace(\", num_iterations=10\", '')\n",
    "    )\n",
    "experiment_path_parts.append(attacks_part)\n",
    "experiment_path = os.path.join(*experiment_path_parts)\n",
    "\n",
    "iteration = 0\n",
    "log_dir = os.path.join(args.log_dir, experiment_path)\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# optimizer\n",
    "optimizer: optim.Optimizer\n",
    "if args.optim == 'sgd':\n",
    "    weight_decay = 1e-4 if (\n",
    "        args.dataset.startswith('imagenet')\n",
    "        or args.dataset == 'bird_or_bicycle'\n",
    "    ) else 2e-4\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=weight_decay)\n",
    "elif args.optim == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "else:\n",
    "    raise ValueError(f'invalid optimizer {args.optim}')\n",
    "\n",
    "lr_drop_epochs = [int(epoch_str) for epoch_str in\n",
    "                  args.lr_schedule.split(',')]\n",
    "\n",
    "# check for checkpoints\n",
    "def get_checkpoint_fnames():\n",
    "    for checkpoint_fname in glob.glob(os.path.join(glob.escape(log_dir),\n",
    "                                                   '*.ckpt.pth')):\n",
    "        epoch = int(os.path.basename(checkpoint_fname).split('.')[0])\n",
    "        if epoch < args.num_epochs:\n",
    "            yield epoch, checkpoint_fname\n",
    "\n",
    "start_epoch = 0\n",
    "latest_checkpoint_epoch = -1\n",
    "latest_checkpoint_fname = None\n",
    "for epoch, checkpoint_fname in get_checkpoint_fnames():\n",
    "    if epoch > latest_checkpoint_epoch:\n",
    "        latest_checkpoint_epoch = epoch\n",
    "        latest_checkpoint_fname = checkpoint_fname\n",
    "if latest_checkpoint_fname is not None:\n",
    "    state = torch.load(latest_checkpoint_fname)\n",
    "    if 'iteration' in state:\n",
    "        iteration = state['iteration']\n",
    "    if isinstance(model, FeatureModel):\n",
    "        model.model.load_state_dict(state['model'])\n",
    "    else:\n",
    "        model.load_state_dict(state['model'])\n",
    "    if 'optimizer' in state:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "    start_epoch = latest_checkpoint_epoch + 1\n",
    "    adaptive_eps = state.get('adaptive_eps', {})\n",
    "\n",
    "# parallelize\n",
    "if torch.cuda.is_available():\n",
    "    device_ids = list(range(args.parallel))\n",
    "    model = nn.DataParallel(model, device_ids)\n",
    "    attacks = [nn.DataParallel(attack, device_ids) for attack in attacks]\n",
    "    validation_attacks = [nn.DataParallel(attack, device_ids)\n",
    "                          for attack in validation_attacks]\n",
    "\n",
    "# necessary to put training loop in a function because otherwise we get\n",
    "# huge memory leaks\n",
    "def run_iter(\n",
    "    inputs: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    iteration: int,\n",
    "    train: bool = True,\n",
    "    log_fn: Optional[Callable[[str, Any], Any]] = None,\n",
    "):\n",
    "    prefix = 'train' if train else 'val'\n",
    "    if log_fn is None:\n",
    "        log_fn = lambda tag, value: writer.add_scalar(\n",
    "            f'{prefix}/{tag}', value, iteration)\n",
    "\n",
    "    model.eval()  # set model to eval to generate adversarial examples\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    # FORWARD PASS\n",
    "    if train:\n",
    "        optimizer.zero_grad()\n",
    "        model.train()  # now we set the model to train mode\n",
    "\n",
    "    logits = model(inputs)\n",
    "\n",
    "    # CONSTRUCT LOSS\n",
    "    loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "    if args.maximize_attack:\n",
    "        loss, _ = loss.resize(len(step_attacks), inputs.size()[0]).max(0)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # LOGGING\n",
    "    accuracy = calculate_accuracy(logits, labels)\n",
    "    log_fn('loss', loss.item())\n",
    "    log_fn('accuracy', accuracy.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for attack_index, attack in enumerate(step_attacks):\n",
    "            if isinstance(attack, nn.DataParallel):\n",
    "                attack_name = attack.module.__class__.__name__\n",
    "            else:\n",
    "                attack_name = attack.__class__.__name__\n",
    "            attack_logits = logits[\n",
    "                attack_index * inputs.size()[0]:\n",
    "                (attack_index + 1) * inputs.size()[0]\n",
    "            ]\n",
    "            log_fn(f'loss/{attack_name}',\n",
    "                   F.cross_entropy(attack_logits, labels).item())\n",
    "            log_fn(f'accuracy/{attack_name}',\n",
    "                   calculate_accuracy(attack_logits, labels).item())\n",
    "\n",
    "    # if train:\n",
    "    #     print(f'ITER {iteration:06d}',\n",
    "    #           f'accuracy: {accuracy.item() * 100:5.1f}%',\n",
    "    #           f'loss: {loss.item():.2f}',\n",
    "    #           sep='\\t')\n",
    "\n",
    "    # OPTIMIZATION\n",
    "    if train:\n",
    "        loss.backward()\n",
    "\n",
    "        # clip gradients and optimize\n",
    "        nn.utils.clip_grad_value_(model.parameters(), args.clip_grad)\n",
    "        optimizer.step()\n",
    "\n",
    "for epoch in range(start_epoch, args.num_epochs):\n",
    "    \n",
    "    print('BEGIN VALIDATION')\n",
    "    model.eval()\n",
    "    if epoch == 0:\n",
    "        evaluation.evaluate_against_attacks(\n",
    "            model, validation_attacks, val_loader, parallel=args.parallel,\n",
    "            writer=writer, iteration=iteration, num_batches=args.val_batches,\n",
    "        )\n",
    "    else:\n",
    "        evaluation.evaluate_against_attacks(\n",
    "            model, [NoAttack()], val_loader, parallel=args.parallel,\n",
    "            writer=writer, iteration=iteration, num_batches=args.val_batches,\n",
    "        )\n",
    "\n",
    "    lr = args.lr\n",
    "    for lr_drop_epoch in lr_drop_epochs:\n",
    "        if epoch >= lr_drop_epoch:\n",
    "            lr *= 0.1\n",
    "    \n",
    "    print(f'START EPOCH {epoch:04d} (lr={lr:.0e})')\n",
    "    ts = time.time()\n",
    "    for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "        # ramp-up learning rate for SGD\n",
    "        if epoch < 5 and args.optim == 'sgd' and args.lr >= 0.1:\n",
    "            lr = (iteration + 1) / (5 * len(train_loader)) * args.lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        run_iter(inputs, labels, iteration)\n",
    "        iteration += 1\n",
    "    print(f'END EPOCH {epoch:04d}', \"time spent:\", time.time()-ts)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # VALIDATION\n",
    "    print('BEGIN VALIDATION')\n",
    "    model.eval()\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        evaluation.evaluate_against_attacks(\n",
    "            model, validation_attacks, val_loader, parallel=args.parallel,\n",
    "            writer=writer, iteration=iteration, num_batches=args.val_batches,\n",
    "        )\n",
    "\n",
    "    checkpoint_fname = os.path.join(log_dir, f'{epoch:04d}.ckpt.pth')\n",
    "    print(f'CHECKPOINT {checkpoint_fname}')\n",
    "    checkpoint_model = model\n",
    "    if isinstance(checkpoint_model, nn.DataParallel):\n",
    "        checkpoint_model = checkpoint_model.module\n",
    "    if isinstance(checkpoint_model, FeatureModel):\n",
    "        checkpoint_model = checkpoint_model.model\n",
    "    state = {\n",
    "        'model': checkpoint_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'iteration': iteration,\n",
    "        'arch': args.arch,\n",
    "    }\n",
    "    torch.save(state, checkpoint_fname)\n",
    "\n",
    "    # delete extraneous checkpoints\n",
    "    last_keep_checkpoint = (epoch // args.keep_every) * args.keep_every\n",
    "    for epoch, checkpoint_fname in get_checkpoint_fnames():\n",
    "        if epoch < last_keep_checkpoint and epoch % args.keep_every != 0:\n",
    "            print(f'  remove {checkpoint_fname}')\n",
    "            os.remove(checkpoint_fname)\n",
    "\n",
    "print('BEGIN EVALUATION')\n",
    "model.eval()\n",
    "\n",
    "evaluation.evaluate_against_attacks(\n",
    "    model, validation_attacks, val_loader, parallel=args.parallel,\n",
    ")\n",
    "print('END EVALUATION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
